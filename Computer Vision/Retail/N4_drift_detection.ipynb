{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6540f10a-6e1c-44ba-b344-0a476491dce6",
   "metadata": {},
   "source": [
    "# Workshop Notebook 3: Observability - Drift Detection\n",
    "\n",
    "In the previous notebook you learned how to add simple validation rules to a pipeline, to monitor whether outputs (or inputs) stray out of some expected range. In this notebook, you will monitor the *distribution* of the pipeline's predictions to see if the model, or the environment that it runs it, has changed.\n",
    "\n",
    "## Preliminaries\n",
    "\n",
    "In the blocks below we will preload some required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d0f316-7000-467e-b5d2-1a1c4e18d490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this to preload needed libraries \n",
    "\n",
    "import wallaroo\n",
    "from wallaroo.object import EntityNotFoundError\n",
    "from wallaroo.framework import Framework\n",
    "from IPython.display import display\n",
    "from IPython.display import Image\n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import random\n",
    "import pyarrow as pa\n",
    "import sys\n",
    "import asyncio\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "import sys\n",
    " \n",
    "# setting path - only needed when running this from the `with-code` folder.\n",
    "sys.path.append('../')\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5414c3f",
   "metadata": {},
   "source": [
    "## Pre-exercise\n",
    "\n",
    "If needed, log into Wallaroo and go to the workspace, pipeline, and most recent model version from the ones that you created in the previous notebook. Please refer to Notebook 1 to refresh yourself on how to log in and set your working environment to the appropriate workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ca3872-038b-4851-bee2-069799ac0cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## blank space to log in \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b1344e",
   "metadata": {},
   "source": [
    "### Upload Computer Vision with Pixel Intensity Model\n",
    "\n",
    "For this example, we'll be using a different Bring Your Own Predict (BYOP) of the same models used before, but this time we have an additional output field:  `avg_px_intensity`. This takes the incoming images and averages out their pixel intensity.  Typically the higher the value, the more blurred and indistinct the image is.\n",
    "\n",
    "We'll upload it with the following.  Adjust the variable names as best fit your preferences.\n",
    "\n",
    "```python\n",
    "input_schema = pa.schema([\n",
    "    pa.field('tensor', pa.list_(\n",
    "        pa.list_(\n",
    "            pa.list_(\n",
    "                pa.float32(), # images are normalized\n",
    "                list_size=640\n",
    "            ),\n",
    "            list_size=480\n",
    "        ),\n",
    "        list_size=3\n",
    "    )),\n",
    "])\n",
    "\n",
    "output_schema = pa.schema([\n",
    "    pa.field('boxes', pa.list_(pa.list_(pa.float32(), list_size=4))),\n",
    "    pa.field('classes', pa.list_(pa.int64())),\n",
    "    pa.field('confidences', pa.list_(pa.float32())),\n",
    "    pa.field('avg_px_intensity', pa.list_(pa.float32())),\n",
    "    pa.field('avg_confidence', pa.list_(pa.float32())),\n",
    "])\n",
    "\n",
    "\n",
    "model = wl.upload_model(\"cv-pixel-intensity\", \n",
    "                        \"models/model-with-pixel-intensity.zip\", \n",
    "                        framework=Framework.CUSTOM,\n",
    "                        input_schema=input_schema, \n",
    "                        output_schema=output_schema)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae7494a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## blank space to upload the observation model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8ae0c7",
   "metadata": {},
   "source": [
    "Create a new pipeline and add the model as our pipeline step.  We'll leave those steps out - you should know them by now.  But we'll include a recommended deployment configuration to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02ad6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this deployment configuration\n",
    "\n",
    "deployment_config = wallaroo.DeploymentConfigBuilder() \\\n",
    "    .replica_count(1) \\\n",
    "    .cpus(1) \\\n",
    "    .memory(\"2Gi\") \\\n",
    "    .sidekick_cpus(model, 1) \\\n",
    "    .sidekick_memory(model, '6Gi') \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4889474f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## blank space to build the pipeline and add the model step, then deploy the pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44f8745",
   "metadata": {},
   "outputs": [],
   "source": [
    "## blank space to check the pipeline status\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4837a42",
   "metadata": {},
   "source": [
    "### Sample Inference\n",
    "\n",
    "For our sample inference, we'll use the file `'./data/dairy_bottles.json'` - what we're looking for is the field `out.avg_px_intensity`.  The following is one method of displaying only that field:\n",
    "\n",
    "```python\n",
    "pipeline.infer_from_file('../data/dairy_bottles.json')[\"out.avg_px_intensity\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c28c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "## blank space to perform sample inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e76d00f-4a25-4c8f-b144-105305a64e6b",
   "metadata": {},
   "source": [
    "## Monitoring for Drift: Shift Happens. \n",
    "\n",
    "In machine learning, you use data and known answers to train a model to make predictions for new previously unseen data. You do this with the assumption that the future unseen data will be similar to the data used during training: the future will look somewhat like the past.\n",
    "But the conditions that existed when a model was created, trained and tested can change over time, due to various factors.\n",
    "\n",
    "A good model should be robust to some amount of change in the environment; however, if the environment changes too much, your models may no longer be making the correct decisions. This situation is known as concept drift; too much drift can obsolete your models, requiring periodic retraining.\n",
    "\n",
    "Let's consider the example we've been working on: home sale price prediction. You may notice over time that there has been a change in the mix of properties in the listings portfolio: for example a dramatic increase or decrease in expensive properties (or more precisely, properties that the model thinks are expensive)\n",
    "\n",
    "Such a change could be due to many factors: a change in interest rates; the appearance or disappearance of major sources of employment; new housing developments opening up in the area. Whatever the cause, detecting such a change quickly is crucial, so that the business can react quickly in the appropriate manner, whether that means simply retraining the model on fresher data, or a pivot in business strategy.\n",
    "\n",
    "In Wallaroo you can monitor your housing model for signs of drift through the model monitoring and insight capability called Assays. Assays help you track changes in the environment that your model operates within, which can affect the model’s outcome. It does this by tracking the model’s predictions and/or the data coming into the model against an **established baseline**. If the distribution of monitored values in the current observation window differs too much from the baseline distribution, the assay will flag it. The figure below shows an example of a running scheduled assay.\n",
    "\n",
    "\n",
    "![](https://docs.wallaroo.ai/images/current/wallaroo-tutorials/wallaroo-tutorial-features/wallaroo-model-insights-reference_files/wallaroo-model-insights-reference_35_0.png)\n",
    "\n",
    "**Figure:** A daily assay that's been running for a month. The dots represent the difference between the distribution of values in the daily observation window, and the baseline. When that difference exceeds the specified threshold (indicated by a red dot) an alert is set.\n",
    "\n",
    "This next set of exercises will walk you through setting up an assay to monitor the predictions of your house price model, in order to detect drift.\n",
    "\n",
    "### NOTE\n",
    "\n",
    "An assay is a monitoring process that typically runs over an extended, ongoing period of time. For example, one might set up an assay that every day monitors the previous 24 hours' worth of predictions and compares it to a baseline. For the purposes of these exercises, we'll be compressing processes what normally would take hours or days into minutes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a15433-71c3-4dc3-88cb-7765b19d6018",
   "metadata": {},
   "source": [
    "## Set Assay Baseline\n",
    "\n",
    "In order to know whether the distribution of your model's predictions have changed, you need a baseline to compare them to. This baseline should represent how you expect the model to behave at the time it was trained. This might be approximated by the distribution of the model's predictions over some \"typical\" period of time. For example, we might collect the predictions of our model over the first few days after it's been deployed. For these exercises, we'll compress that to a few minutes. Currently, to set up a wallaroo assay the pipeline must have been running for some period of time, and the assumption is that this period of time is \"typical\", and that the distributions of the inputs and the outputs of the model during this period of time are \"typical.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630f32da-6601-44fd-bf8c-c7df850f7b77",
   "metadata": {},
   "source": [
    "## Create Set Baseline\n",
    "\n",
    "Before setting up an assay on this pipeline's output, we may want to look at the distribution of the predictions over our selected baseline period. First we'll perform a set of inferences and use that data to create our baseline values.  These are either a date period of inferences to look at, or a numpy array generated from the sample data.\n",
    "\n",
    "### Create Set Baseline Example\n",
    "\n",
    "Create an assay builder to monitor the output of your house price pipeline. The baseline period should be from `baseline_start` to `baseline_end`. \n",
    "\n",
    "* You will need to know the name of your output variable, and the name of the model in the pipeline.  In our case, we will be relying on the post processing Python model we deployed with the pipeline.\n",
    "\n",
    "Here's an example.\n",
    "\n",
    "```python\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "assay_baseline_start = datetime.datetime.now()\n",
    "\n",
    "baseline_start = datetime.datetime.now(datetime.timezone.utc)\n",
    "\n",
    "## Blank space to create an assay builder and examine the baseline distribution\n",
    "\n",
    "blurred_images = [\n",
    "    '../data/blurred_dairy_bottles.json',\n",
    "    \"../data/blurred_dairy_products.json\",\n",
    "    \"../data/blurred_product_cheeses.json\",\n",
    "]\n",
    "\n",
    "\n",
    "baseline_images = [\n",
    "    \"../data/dairy_bottles.json\",\n",
    "    \"../data/dairy_products.json\",\n",
    "    \"../data/product_cheeses.json\",\n",
    "]\n",
    "\n",
    "for image in baseline_images:\n",
    "    with open(image, 'r') as file:\n",
    "        df = pd.read_json(image)\n",
    "    pipeline.infer(df)\n",
    "\n",
    "time.sleep(60)\n",
    "assay_baseline_end = datetime.datetime.now()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c8c723-9b10-4839-a330-1c78d0f4e145",
   "metadata": {},
   "outputs": [],
   "source": [
    "## blank space to set the baseline data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7543c1fb",
   "metadata": {},
   "source": [
    "## Create Assay\n",
    "\n",
    "Baselines are created with the `wallaroo.client.build_assay` method and take the following arguments.\n",
    "\n",
    "| Parameter | Type | Description |\n",
    "|---|---|---|\n",
    "| **assay_name** | (*String*) (*Required*) | The name of the assay.  Assay names **must** be unique across the Wallaroo instance. |\n",
    "| **pipeline** | (*wallaroo.pipeline*) (*Required*) | The pipeline the assay retrieves data from. |\n",
    "| **model_name** | (*String*) (*Required*) | The name of the model.  This must be one of the models deployed in the pipeline. |\n",
    "| **iopath** | (*String*) (*Required*) | The input/output path of \"output|input {field_name} {field_index}\".  For example, `\"output avg_conf 0\"` to use the model's outputs field `avg_conf` at index `0` |\n",
    "| **baseline_start** | (*datetime.datetime*) (*Optional*) | The start time for the inferences to use as the baseline.  If `baseline_start` is used, it **must** be paired with `baseline_end` and **not** `baseline_data`.\n",
    "| **baseline_end** | (*datetime.datetime*) (*Optional*) | The end time of the baseline window. the baseline. Windows start immediately after the baseline window and are run at regular intervals continously until the assay is deactivated or deleted.  If `baseline_end` is used, it **must** be paired with `baseline_start` and **not** `baseline_data`.\n",
    "| **baseline_data** | (*numpy.array*) (*Optional*) | The baseline data in numpy array format.  If `baseline_data` is used, `baseline_start` and `baseline_end` must **not** be used. |\n",
    "\n",
    "Baseline data **must** be either `baseline_start` **and** `baseline_end`, or **baseline_data**.\n",
    "\n",
    "### Create Assay Exercise\n",
    "\n",
    "For this exercise, we will create the assay, then display the baseline histogram from the assay.\n",
    "\n",
    "Sample code:\n",
    "\n",
    "```python\n",
    "assay_baseline_from_dates = wl.build_assay(assay_name=\"assays from date baseline\", \n",
    "                               pipeline=pipeline, \n",
    "                               iopath=\"output avg_px_intensity 0\",\n",
    "                               baseline_start=assay_baseline_start,\n",
    "                               baseline_end=assay_baseline_end)\n",
    "\n",
    "\n",
    "# create the baseline from the dates\n",
    "assay_config_from_dates = assay_baseline_from_dates.build()\n",
    "assay_results_from_dates = assay_config_from_dates.interactive_run()\n",
    "\n",
    "assay_baseline_from_dates.baseline_histogram()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231c050c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## blank space to create the assay baseline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdafee0-e6eb-4306-a498-3d816a8a2600",
   "metadata": {},
   "source": [
    "## Assay Windows\n",
    "\n",
    "An assay should detect if the distribution of model predictions changes from the above distribution over regularly sampled observation windows. This is called *drift*.\n",
    "\n",
    "To show drift, we'll run more data through the pipeline -- first some data drawn from the same distribution as the baseline (`lowprice_data`). Then, we will gradually introduce more data from a different distribution (`highprice_data`). We should see the difference between the baseline distribution and the distribution in the observation window increase.\n",
    "\n",
    "To set up the data, you should do something like the below. It will take a while to run, because of all the `sleep` intervals.\n",
    "\n",
    "You will need the `assay_window_end` for a later exercise.\n",
    "\n",
    "**IMPORTANT NOTE**:  To generate the data for the assay, this process may take 4-5 minutes.  Because the shortest period of time for an assay window is 1 minute, the intervals of inference data are spaced to fall within that time period.\n",
    "\n",
    "```python\n",
    "\n",
    "## Blank space to create an assay builder and examine the baseline distribution\n",
    "\n",
    "# run a set of blurred images\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "assay_window_start = datetime.datetime.now(datetime.timezone.utc)\n",
    "time.sleep(60)\n",
    "\n",
    "for image in blurred_images:\n",
    "    with open(image, 'r') as file:\n",
    "        df = pd.read_json(image)\n",
    "    pipeline.infer(df)\n",
    "\n",
    "time.sleep(60)\n",
    "\n",
    "# run a set of normal images\n",
    "\n",
    "for image in baseline_images:\n",
    "    with open(image, 'r') as file:\n",
    "        df = pd.read_json(image)\n",
    "    pipeline.infer(df)\n",
    "\n",
    "time.sleep(60)\n",
    "assay_window_end = datetime.datetime.now(datetime.timezone.utc)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee23c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## blank space to  create an assay builder and examine the baseline distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51133baa-e0c8-41ca-a781-fdab6e1de4d9",
   "metadata": {},
   "source": [
    "## Define Assay Parameters\n",
    "\n",
    "Now we're finally ready to set up an assay!\n",
    "\n",
    "### The Observation Window\n",
    "\n",
    "Once a baseline period has been established, you must define the window of observations that will be compared to the baseline. For instance, you might want to set up an assay that runs *every 12 hours*, collects the *previous 24 hours' predictions* and compares the distribution of predictions within that 24 hour window to the baseline. To set such a comparison up would look like this:\n",
    "\n",
    "```python\n",
    "assay_builder.window_builder().add_width(hours=24).add_interval(hours=12)\n",
    "```\n",
    "\n",
    "In other words **_width_** is the width of the observation window, and **_interval_** is how often an assay (comparison) is run. The default value of *width* is 24 hours; the default value of *interval* is to set it equal to *width*. The units can be specified in one of: `minutes`, `hours`, `days`, `weeks`.\n",
    "\n",
    "### The Comparison Threshold\n",
    "Given an observation window and a baseline distribution, an assay computes the distribution of predictions in the observation window. It then calculates the \"difference\" (or \"distance\") between the observed distribution and the baseline distribution. For the assay's default distance metric (which we will use here), a good starting threshold is 0.1. Since a different value may work best for a specific situation, you can try interactive assay runs on historical data to find a good threshold, as we do in these exercises.\n",
    "\n",
    "To set the assay threshold for the assays to 0.1:\n",
    "\n",
    "```python\n",
    "assay_builder.add_alert_threshold(0.1)\n",
    "```\n",
    "\n",
    "### Running an Assay on Historical Data\n",
    "\n",
    "In this exercise, you will build an **interactive assay** over historical data. To do this, you need an end time (`endtime`). \n",
    "\n",
    "Depending on the historical history, the window and interval may need adjusting.  If using the previously generated information, an interval window as short as 1 minute may be useful.\n",
    "\n",
    "Assuming you have an assay builder with the appropriate window parameters and threshold set, you can do an interactive run and look at the results would look like this.\n",
    "\n",
    "By default, assay start date is set to 24 hours from when the assay was created.  For this example, we will set the `assay.window_builder.add_start` to set the assay window to start at the beginning of our data, and `assay.add_run_until` to set the time period to stop gathering data from.\n",
    "\n",
    "```python\n",
    "# set the end of the interactive run\n",
    "assay_builder.add_run_until(endtime)\n",
    "\n",
    "# set the window\n",
    "\n",
    "assay_builder.window_builder().add_width(hours=24).add_interval(hours=12).add_start(start_time)\n",
    "\n",
    "assay_results = assay_builder.build().interactive_run()\n",
    "df = assay_results.to_dataframe() # to return the results as a table\n",
    "assay_results.chart_scores() # to plot the run\n",
    "```\n",
    "\n",
    "### Define Assay Parameters Exercise\n",
    "\n",
    "Use the assay_builder you created in the previous exercise to set up an interactive assay. \n",
    "* The assay should run every minute, on a window that is a minute wide. \n",
    "* Set the alert threshold to 0.1.  \n",
    "* You can use `assay_window_end` (or a later timestamp) as the end of the interactive run.\n",
    "\n",
    "Examine the assay results. Do you see any drift?\n",
    "\n",
    "To try other ways of examining the assay results, see the [\"Interactive Assay Runs\" section of the Model Insights tutorial](https://docs.wallaroo.ai/wallaroo-tutorials/wallaroo-tutorial-features/wallaroo-model-insights/#interactive-assay-runs).\n",
    "\n",
    "Here's some code to use.\n",
    "\n",
    "```python\n",
    "assay_baseline_from_dates = wl.build_assay(assay_name=\"assays from date baseline\", \n",
    "                               pipeline=pipeline, \n",
    "                               iopath=\"output avg_px_intensity 0\",\n",
    "                               baseline_start=assay_baseline_start,\n",
    "                               baseline_end=assay_baseline_end)\n",
    "\n",
    "\n",
    "# # The end date to gather inference results\n",
    "assay_baseline_from_dates.add_run_until(datetime.datetime.now(datetime.timezone.utc))\n",
    "\n",
    "# # Set the interval and window to one minute each, set the start date for gathering inference results\n",
    "assay_baseline_from_dates.window_builder().add_width(minutes=1).add_interval(minutes=1).add_start(assay_window_start)\n",
    "\n",
    "\n",
    "# create the baseline from the dates\n",
    "assay_config_from_dates = assay_baseline_from_dates.build()\n",
    "assay_results_from_dates = assay_config_from_dates.interactive_run()\n",
    "\n",
    "\n",
    "df = assay_results_from_dates.to_dataframe()\n",
    "display(assay_results_from_dates.chart_scores())\n",
    "display(df)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920ede66-5919-4293-9297-7c1a06fef34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## blank space to run the assay with adjusted \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac0b634-2824-4ae3-a5cf-a15c548a7af2",
   "metadata": {},
   "source": [
    "## Schedule an Assay for Ongoing Data\n",
    "\n",
    "(We won't be doing an exercise here, this is for future reference).\n",
    "\n",
    "Once you are satisfied with the parameters you have set, you can schedule an assay to run regularly .\n",
    "\n",
    "```python\n",
    "assay_baseline_from_dates = wl.build_assay(assay_name=\"assays from date baseline\", \n",
    "                               pipeline=pipeline, \n",
    "                               iopath=\"output avg_px_intensity 0\",\n",
    "                               baseline_start=assay_baseline_start,\n",
    "                               baseline_end=assay_baseline_end)\n",
    "\n",
    "\n",
    "# # The end date to gather inference results\n",
    "assay_baseline_from_dates.add_run_until(datetime.datetime.now(datetime.timezone.utc))\n",
    "\n",
    "# # Set the interval and window to one minute each, set the start date for gathering inference results\n",
    "assay_baseline_from_dates.window_builder().add_width(minutes=1).add_interval(minutes=1).add_start(assay_window_start)\n",
    "\n",
    "# now schedule the assay\n",
    "assay_id = assay_baseline_from_dates.upload()\n",
    "```\n",
    "\n",
    "You can use the assay id later to get the assay results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf4dd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assay_id = assay_builder.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c643514e",
   "metadata": {},
   "source": [
    "## Cleaning up.\n",
    "\n",
    "Now that the workshop is complete, don't forget to undeploy your pipeline to free up the resources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371b9334",
   "metadata": {},
   "outputs": [],
   "source": [
    "## blank space to undeploy your pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6179b95-b0bc-4057-80ee-aa8d543f40ac",
   "metadata": {},
   "source": [
    "You have now walked through setting up a basic assay and running it over historical data.\n",
    "\n",
    "## Congratulations!\n",
    "\n",
    "In this workshop you have\n",
    "* Deployed a single step house price prediction pipeline and sent data to it.\n",
    "* Compared two house price prediction models in an A/B test\n",
    "* Compared two house price prediction models in a shadow deployment.\n",
    "* Swapped the \"winner\" of the comparisons into the house price prediction pipeline.\n",
    "* Set validation rules on the pipeline.\n",
    "* Set up an assay on the pipeline to monitor for drift in its predictions.\n",
    "\n",
    "Great job!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wallaroosdk2024.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
