{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "842531aa-4f9f-41fc-884e-8bce107a19ee",
   "metadata": {},
   "source": [
    "# Workshop Notebook 7: Deploy Pipeline to Edge Devices\n",
    "\n",
    "For this workshop, we will take a Wallaroo pipeline and publish it to an Open Container (OCI) Registry.  The registry details are stored in the Wallaroo instance as the Edge Registry.  \n",
    "\n",
    "In this set of exercises, you will:\n",
    "\n",
    "1. Use a pre-trained model and deploy it to Wallaroo.\n",
    "1. Perform sample inferences.\n",
    "1. Publish the pipeline to the Edge Registry.\n",
    "1. See the steps to deploy the published pipeline to an Edge device and perform inferences through it.\n",
    "\n",
    "Deployment to the Edge allows data scientists to work in Wallaroo to test their models in Wallaroo, then once satisfied with the results publish those pipelines.  DevOps engineers then take those published pipeline details from the Edge registry and deploy them into Docker and Kubernetes environments.\n",
    "\n",
    "This workshop will demonstrate the following concepts:\n",
    "\n",
    "* [Wallaroo Workspaces](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-essentials-workspace/):  Workspaces are environments were users upload models, create pipelines and other artifacts.  The workspace should be considered the fundamental area where work is done.  Workspaces are shared with other users to give them access to the same models, pipelines, etc.\n",
    "* [Wallaroo Model Upload and Registration](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-model-uploads/): ML Models are uploaded to Wallaroo through the SDK or the MLOps API to a **workspace**.  ML models include default runtimes (ONNX, Python Step, and TensorFlow) that are run directly through the Wallaroo engine, and containerized runtimes (Hugging Face, PyTorch, etc) that are run through in a container through the Wallaroo engine.\n",
    "* [Wallaroo Pipelines](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-essentials-pipelines/): Pipelines are used to deploy models for inferencing.  Each model is a **pipeline step** in a pipelines, where the inputs of the previous step are fed into the next.  Pipeline steps can be ML models, Python scripts, or Arbitrary Python (these contain necessary models and artifacts for running a model).\n",
    "* [Pipeline Edge Publication](https://staging.docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-essentials-pipelines/wallaroo-sdk-essentials-pipeline-publication/): How to publish a Wallaroo pipeline to an OCI registry, then deploy that pipeline into other environments.\n",
    "\n",
    "For this tutorial, we will be providing pre-trained models in ONNX format, and have connected a sample Edge Registry to our Wallaroo instance.\n",
    "\n",
    "For more Wallaroo procedures, see the [Wallaroo Documentation site](https://docs.wallaroo.ai)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1390c8",
   "metadata": {},
   "source": [
    "\n",
    "Before we start, let's load some libraries that we will need for this notebook (note that this may not be a complete list).\n",
    "\n",
    "* **IMPORTANT NOTE**:  This tutorial is geared towards a Wallaroo 2023.3.0 environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb03a96b-3d13-4d5e-9aee-99913d9a37ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preload needed libraries \n",
    "\n",
    "import wallaroo\n",
    "from wallaroo.object import EntityNotFoundError\n",
    "from wallaroo.framework import Framework\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "# used to display DataFrame information without truncating\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "import json\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "# used for unique connection names\n",
    "\n",
    "import string\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74c5788-1b13-44b5-9f2c-82fdc531df78",
   "metadata": {},
   "source": [
    "\n",
    "## Get ready to work with Wallaroo\n",
    "\n",
    "With the libraries loaded, you can log into Wallaroo.  This will provide access to your workspaces, workspaces shared with you from other users, and all other aspects of the Wallaroo environment.\n",
    "\n",
    "Logging into Wallaroo via the cluster's integrated JupyterLab is quite straight forward:\n",
    "\n",
    "```python\n",
    "# Login through local Wallaroo instance \n",
    "wl = wallaroo.Client()\n",
    "```\n",
    "\n",
    "See [the documentation](https://docs.wallaroo.ai/wallaroo-101/#connect-to-the-wallaroo-instance) if you are logging into Wallaroo some other way such as from a remote location.  This tutorial assumes you're logging in through the Wallaroo JupyterHub service.\n",
    "\n",
    "Notice that the Wallaroo client connection is stored into a variable called **wl**.  This variable can be anything you want it to be - you can have `client = wallaroo.Client()` or `myWallarooClient = wallaroo.Client()`.\n",
    "\n",
    "This variable is about to become your best friend - a lot of the commands you'll be running will be through this variable, like `wl.list_workspaces()` to get all of the workspaces available to you in your Wallaroo environment, or `wl.list_models()` to show all of the models in your current workspace.  We'll go into these commands and more - just make sure you saved that Wallaroo client to a variable so you can use it for the other commands.\n",
    "\n",
    "When we log into the Wallaroo through the SDK, the Client will provide a url to verify your authentication.  Either click it, or copy and past that URL, then authenticate into your Wallaroo instance with your email address and password.\n",
    "\n",
    "### Login to Wallaroo Exercise\n",
    "\n",
    "Time to login to your Wallaroo instance.  By now you should be logged into your Wallaroo JupyterHub service and looking at this notebook.\n",
    "\n",
    "Copy the code below and place it into the code block and run it.  When prompted, select the authentication URL by either clicking it, or copying and pasting it into a browser.  Log into your Wallaroo instance, and then the client will be set.\n",
    "\n",
    "```python\n",
    "# Login through local Wallaroo instance \n",
    "wl = wallaroo.Client()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f89932e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your wallaroo Client login code here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf1e30d",
   "metadata": {},
   "source": [
    "### Retrieve or Create Workspace\n",
    "\n",
    "This tutorial assumes that the user is already experienced with creating workspaces, pipeline, and models from the first notebook \"Build and Deploy a Model\".  To speed this process along, the following helper functions are used to set the workspace or retrieve an existing one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8192998c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the workspace called <name>, or create it if it does not exist.\n",
    "# this function assumes your connection to wallaroo is called wl\n",
    "def get_workspace(name, client):\n",
    "    workspace = None\n",
    "    for ws in client.list_workspaces():\n",
    "        if ws.name() == name:\n",
    "            workspace= ws\n",
    "    if(workspace == None):\n",
    "        workspace = client.create_workspace(name)\n",
    "    return workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a40a29",
   "metadata": {},
   "source": [
    "#### Retrieve Workspace Exercise\n",
    "\n",
    "Using the helper function `get_workspace(workspace_name, client)`, create or retrieve the workspace `workshop-workspace-{firstname}`, replacing `{firstname}` with your first name.  If two users in this training session have the same first name, decide between each other what name to use.\n",
    "\n",
    "Store the workspace into the variable `workspace`.  Once created or retrieved, set that workspace as your **current workspace**.\n",
    "\n",
    "For example, the following code would create the workspace `workshop-workspace-john` and set it as the current workspace, then display our current workspace to verify the process worked.\n",
    "\n",
    "```python\n",
    "workspace = get_workspace('workshop-workspace-john', wl)\n",
    "\n",
    "wl.set_current_workspace(workspace)\n",
    "wl.get_current_workspace()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0851a47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blank space for retrieving the workspace\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b7b6f7-696f-436e-acde-2f50b361b8a1",
   "metadata": {},
   "source": [
    "## Upload a Model\n",
    "\n",
    "This process was covered perviously in \"Build and Deploy a Model\", so just a quick recap.  Models are uploaded with the `wallaroo.Client.upload_model` method.\n",
    "\n",
    "Wallaroo supports ONNX models as part of the default runtime, so these will run in Wallaroo without additional configurations.\n",
    "\n",
    "When uploading models, the following is needed:\n",
    "\n",
    "* The **name** of the model.  This needs to be unique across the **workspace**.  \n",
    "* The **path** to the ML model file.  For example, `./models/xgb_model.onnx`.\n",
    "* The **framework** of the model.  These are listed through the `wallaroo.framework.Framework` list.  For these examples we will be using `wallaroo.framework.Framework.ONNX` to specify we are using ONNX models.\n",
    "* The **input_schema** and **output_schema**.  For ONNX models, we can skip this.  For non-native runtime models, that has to be specified in Apache Arrow schema format.  See the [Wallaroo SDK Essentials Guide: Model Uploads and Registrations](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-model-uploads/) for full details.\n",
    "\n",
    "Because our sample model is in the ONNX framework, we will not have to specify the input or output schemas.\n",
    "\n",
    "For full details, see [Wallaroo SDK Essentials Guide: Model Uploads and Registrations](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-model-uploads/).\n",
    "\n",
    "### Upload a Model Exercise\n",
    "\n",
    "For this exercise, upload the model `/models/xgb_model.onnx` and assign it a name, with the framework `=wallaroo.framework.Framework.ONNX`.  For example, if the Wallaroo client is saved to the variable `wl` and we want to name out model `house-price-prime`, we would do the following to store the model version we are uploading to the `model_version` variable.\n",
    "\n",
    "```python\n",
    "my_model_version = wl.upload_model('house-price-prime',\n",
    "                './models/xgb_model.onnx',\n",
    "                framework=wallaroo.framework.Framework.ONNX)\n",
    "# display the model version information\n",
    "my_model_version\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e186d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# blank space to upload the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335b77a7",
   "metadata": {},
   "source": [
    "## Build a Pipeline\n",
    "\n",
    "Pipelines are the method of taking submitting data and processing that data through the models. Each pipeline can have one or more **steps** that submit the data from the previous step to the next one. Information can be submitted to a pipeline as a file, or through the pipeline’s URL.\n",
    "\n",
    "This process was covered in detail in \"Build and Deploy a Model\".  See [Wallaroo SDK Essentials Guide: Pipeline Management](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-essentials-pipelines/wallaroo-sdk-essentials-pipeline/) for full details.\n",
    "\n",
    "### Build a Pipeline Exercise\n",
    "\n",
    "Use the `wallaroo.Client.build_pipeline(name)` command and create a pipeline named `houseprice-pipeline`.  Recall that this creates the pipeline in the **current workspace**, so verify that the current workspace is the one you want to create a pipeline in.  Store the pipeline into a variable labeled `my_pipeline`\n",
    "\n",
    "For example, if the Wallaroo client is saved to the variable `wl`, the command would be:\n",
    "\n",
    "```python\n",
    "my_pipeline = wl.build_pipeline('houseprice-estimator')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b16e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# blank space for you to create the pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b8be75",
   "metadata": {},
   "source": [
    "## Add Model Step\n",
    "\n",
    "Models are added to a pipeline as pipeline steps.  There are different kinds of pipeline steps that can host one or more models.\n",
    "\n",
    "For this workshop, we will use the method `wallaroo.pipeline.add_model_step(model_version)`.  This adds a single step to a Pipeline.  Pipeline steps start at 0 and increment from there.  We can see the steps in our pipeline with the `wallaroo.pipeline.steps()` method.\n",
    "\n",
    "```python\n",
    "# add modelA as a pipeline steps\n",
    "pipeline.add_model_step(modelA)\n",
    "\n",
    "# display the steps\n",
    "pipeline.steps()\n",
    "\n",
    "[{'ModelInference': {'models': [{'name': 'house-price-prime', 'version': 'c86fd309-7c28-4e95-9d3e-831fefa51a12', 'sha': 'e22a0831aafd9917f3cc87a15ed267797f80e2afa12ad7d8810ca58f173b8cc6'}]}}]\n",
    "```\n",
    "\n",
    "This process was covered in the notebook \"Build and Deploy a Model\".  For full details, see [Wallaroo SDK Essentials Guide: Pipeline Management:Pipeline Steps](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-essentials-pipelines/wallaroo-sdk-essentials-pipeline/#pipeline-steps)\n",
    "\n",
    "### Add Pipeline Step Exercise\n",
    "\n",
    "We have our model version uploaded from the previous steps, and we have our pipeline.  Time to put them together and create a pipeline step with our model version.\n",
    "\n",
    "Just for practice, do the following:\n",
    "\n",
    "1. Clear the pipeline steps.\n",
    "1. Add the sample model uploaded earlier.  In our examples, that was `my_model_version`.\n",
    "1. Show the current pipeline steps.\n",
    "\n",
    "Here's an example with the Wallaroo client stored to `wl`, with the pipeline `my_pipeline` and `my_model_version`:\n",
    "\n",
    "```python\n",
    "my_pipeline.clear()\n",
    "my_pipeline.add_model_step(my_model_version)\n",
    "my_pipeline.steps()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a993ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the pipeline steps\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27e9fc3",
   "metadata": {},
   "source": [
    "## Deploy a Pipeline\n",
    "\n",
    "Now we reach what we've been aiming for:  deploying a pipeline.\n",
    "\n",
    "By now, you've seen how workspaces contain the models, pipelines, and other artifacts.  You've uploaded a model and retrieved the latest version of the model.  You've built a pipeline and added the model version as a pipeline step.\n",
    "\n",
    "Now we will deploy the pipeline.  Deploying a pipeline allocated resources from the cluster to that pipeline for it's use.  The amount of resources has a default value of 4 CPUs, but for this workshop we'll be adjusting that to just 0.5 cpus and 1 GB RAM per pipeline.\n",
    "\n",
    "* **IMPORTANT NOTE**:  Please stick to these resource configurations when using a Wallaroo instance with other users.  Otherwise, pipelines might not deploy if the available resources are used up.\n",
    "\n",
    "To deploy a pipeline, we do two things:\n",
    "\n",
    "* Create a deployment configuration: This is an optional step, but we will make it mandatory for this workshop to allow other users to work in the same Wallaroo instance without running out of resources.\n",
    "* Deploy the pipeline with the deployment configuration:  This is the active step that saves the pipeline steps, and allocates system resources to the pipeline for performing inferences.\n",
    "\n",
    "Deployment configurations are made with the `wallaroo.DeploymentConfigBuilder()` class, and then we assign the resource settings from there.  This is saved to a variable so we can apply it to our pipeline deployment.\n",
    "\n",
    "Here's an example of setting up a deployment with just 0.5 cpu and 1Gi RAM:\n",
    "\n",
    "```python\n",
    "deploy_config = wallaroo.DeploymentConfigBuilder().replica_count(1).cpus(0.5).memory(\"1Gi\").build()\n",
    "```\n",
    "\n",
    "Notice the `replica_count(1)` configuration - this tells Wallaroo to only spin up one replica for this pipeline.  In a production environment, we could spin multiple replicas either manually or automatically as more resources are needed to improve performance.\n",
    "\n",
    "Now we deploy the pipeline with our deployment configuration with the `wallaroo.pipeline.deploy(deploy_configuration)` method.  If our pipeline variable is `my_pipeline`, then we would deploy it as follows:\n",
    "\n",
    "```python\n",
    "deploy_config = wallaroo.DeploymentConfigBuilder().replica_count(1).cpus(0.5).memory(\"1Gi\").build()\n",
    "my_pipeline.deploy(deployment_config=deploy_config)\n",
    "```\n",
    "\n",
    "We can check the status of the pipeline deployment with the `wallaroo.pipeline.status()` method:\n",
    "\n",
    "```python\n",
    "my_pipeline.status()\n",
    "\n",
    "{'status': 'Running',\n",
    " 'details': [],\n",
    " 'engines': [{'ip': '10.244.3.83',\n",
    "   'name': 'engine-6d4fccf5cb-dmwfl',\n",
    "   'status': 'Running',\n",
    "   'reason': None,\n",
    "   'details': [],\n",
    "   'pipeline_statuses': {'pipelines': [{'id': 'houseprice-estimator',\n",
    "      'status': 'Running'}]},\n",
    "   'model_statuses': {'models': [{'name': 'house-price-prime',\n",
    "      'version': 'c86fd309-7c28-4e95-9d3e-831fefa51a12',\n",
    "      'sha': 'e22a0831aafd9917f3cc87a15ed267797f80e2afa12ad7d8810ca58f173b8cc6',\n",
    "      'status': 'Running'}]}}],\n",
    " 'engine_lbs': [{'ip': '10.244.4.100',\n",
    "   'name': 'engine-lb-584f54c899-sswnw',\n",
    "   'status': 'Running',\n",
    "   'reason': None,\n",
    "   'details': []}],\n",
    " 'sidekicks': []}\n",
    "```\n",
    "\n",
    "### Deploy a Pipeline Exercise\n",
    "\n",
    "This exercise will have you deploy your pipeline with the deployment settings we listed above.  For example, if your pipeline was called `my_pipeline`, then your deployment will look like this:\n",
    "\n",
    "```python\n",
    "deploy_config = wallaroo.DeploymentConfigBuilder().replica_count(1).cpus(0.5).memory(\"1Gi\").build()\n",
    "my_pipeline.deploy(deployment_config=deploy_config)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3727ad65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deploy the pipeline and check the status\n",
    "\n",
    "deploy_config = wallaroo.DeploymentConfigBuilder().replica_count(1).cpus(0.5).memory(\"1Gi\").build()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824c932d",
   "metadata": {},
   "source": [
    "## Pipeline Inference with Files\n",
    "\n",
    "Wallaroo deployed pipelines accept three types of data:\n",
    "\n",
    "* JSON\n",
    "* pandas DataFrames\n",
    "* Apache Arrow\n",
    "\n",
    "We do this with one of two commands on a **deployed** pipeline.\n",
    "\n",
    "* `wallaroo.pipeline.infer(input)`: Submits either JSON, a DataFrame, or Apache Arrow to the pipeline for inferences.\n",
    "* `wallaroo.pipeline.infer_from_file(path)`: Submits either a JSON, a DataFrame in pandas Record format, or an Apache Arrow binary file inferences.\n",
    "\n",
    "We'll start with a single input file:  `./data/singleton.df.json`, which contains input data as a `tensor`:\n",
    "\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"tensor\": [\n",
    "            4.0,\n",
    "            3.0,\n",
    "            3710.0,\n",
    "            20000.0,\n",
    "            2.0,\n",
    "            0.0,\n",
    "            2.0,\n",
    "            5.0,\n",
    "            10.0,\n",
    "            2760.0,\n",
    "            950.0,\n",
    "            47.6696014404,\n",
    "            -122.2610015869,\n",
    "            3970.0,\n",
    "            20000.0,\n",
    "            79.0,\n",
    "            0.0,\n",
    "            0.0\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "When we use `infer_from_file`, Wallaroo determines whether the file submitted is one of the three types above, then submits it to the pipeline to perform an inference request.\n",
    "\n",
    "The data received through the SDK is always of the same type submitted:  Submit a DataFrame, get a DataFrame with the data back.  Submit an Arrow table file, get an Arrow table back.  Here's an example of submitting our sample file through a pipeline saved to the variable `pipeline`: \n",
    "\n",
    "```python\n",
    "result = pipeline.infer_from_file('./data/singleton.df.json')\n",
    "display(result)\n",
    "```\n",
    "\n",
    "| | time | in.tensor | out.variable | check_failures\n",
    "---|---|---|---|---|\n",
    "0 | 2023-08-23 15:02:41.452 | [4.0, 3.0, 3710.0, 20000.0, 2.0, 0.0, 2.0, 5.0, 10.0, 2760.0, 950.0, 47.6696014404, -122.2610015869, 3970.0, 20000.0, 79.0, 0.0, 0.0] | [1514079.4] | 0\n",
    "\n",
    "Let's break down each of these fields:\n",
    "\n",
    "* Index (unnamed):  This field doesn't have a label, but in the example above this is the `index`.  We only have one submission, so we have one result.  If we had 20 inputs, we'd have 20 inference results, and each result would be aligned with each row we sent as an input.\n",
    "* **time**: The date and time the inference request.\n",
    "* **in.{variable}**: Every input to the inference request is listed as `in.{variable_name}`.  For example, if our inputs were `house_size_in_square_feet` and `year_house_built`, then the inputs would be listed as `in.house_size_in_square_feet` and `in.year_house_built`.\n",
    "* **out.{variable}**:  Every output to the inference is listed as `out.{variable_name}`.  Our sample model outputs just one output:  `variable`, but others such might output `estimated_house_price`, `initial_offer_price`, and in the inference result those would be listed as `out.estimated_house_price` and `out.initial_offer_price`.\n",
    "* **check_failures**: Indicates if any validation checks failed.  This is covered in later sessions.\n",
    "\n",
    "There's additional data that an inference has that is retrieved by requesting it.  For full details, see the [Wallaroo SDK Essentials Guide: Inference Management](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-essentials-inferences/).\n",
    "\n",
    "## Pipeline Inference with Files Exercise\n",
    "\n",
    "Let's do an inference with two files:\n",
    "\n",
    "* `./data/singleton.df.json`: Inference input in DataFrame records format with one input.\n",
    "* `./data/test_data.df.json`: Inference input in DataFrame records format with over 4,000 inputs.\n",
    "\n",
    "Use each and perform an inference request through your deployed pipeline with the `infer_from_file` method.  For example, if your pipeline is `my_pipeline`, here's the sample inference requests:\n",
    "\n",
    "```python\n",
    "single_result = my_pipeline.infer_from_file('./data/singleton.df.json')\n",
    "display(single_result)\n",
    "\n",
    "multiple_result = my_pipeline.infer_from_file('./data/test_data.df.json')\n",
    "display(multiple_result)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb9cc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample inferences\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8cfeaa-acae-4c99-bd81-c9164945943e",
   "metadata": {},
   "source": [
    "## Undeploying Your Pipeline\n",
    "\n",
    "You should always undeploy your pipelines when you are done with them, or don't need them for a while. This releases the resources that the pipeline is using for other processes to use. You can always redeploy the pipeline when you need it again. As a reminder, here are the commands to deploy and undeploy a pipeline:\n",
    "\n",
    "```python\n",
    "\n",
    "# \"turn off\" the pipeline and releaase its resources\n",
    "my_pipeline.undeploy()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44a754b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# blank space to undeploy the pipeline\n",
    "my_pipeline.undeploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad9ceb5",
   "metadata": {},
   "source": [
    "## Publish the Pipeline for Edge Deployment\n",
    "\n",
    "It worked! For a demo, we'll take working once as \"tested\". So now that we've tested our pipeline, we are ready to publish it for edge deployment.\n",
    "\n",
    "Publishing it means assembling all of the configuration files and model assets and pushing them to an Open Container Initiative (OCI) repository set in the Wallaroo instance as the Edge Registry service.  DevOps engineers then retrieve that image and deploy it through Docker, Kubernetes, or similar deployments.\n",
    "\n",
    "See [Edge Deployment Registry Guide](https://staging.docs.wallaroo.ai/wallaroo-operations-guide/wallaroo-configuration/wallaroo-edge-deployment/) for details on adding an OCI Registry Service to Wallaroo as the Edge Deployment Registry.\n",
    "\n",
    "This is done through the SDK command `wallaroo.pipeline.publish(deployment_config)` which has the following parameters and returns.\n",
    "\n",
    "#### Publish a Pipeline Parameters\n",
    "\n",
    "The `publish` method takes the following parameters.  The containerized pipeline will be pushed to the Edge registry service with the model, pipeline configurations, and other artifacts needed to deploy the pipeline.\n",
    "\n",
    "| Parameter | Type | Description |\n",
    "|---|---|---|\n",
    "| `deployment_config` | `wallaroo.deployment_config.DeploymentConfig` (*Optional*) | Sets the pipeline deployment configuration.  For example:    For more information on pipeline deployment configuration, see the [Wallaroo SDK Essentials Guide: Pipeline Deployment Configuration]({{<ref \"wallaroo-sdk-essentials-pipeline-deployment-config\">}}).\n",
    "\n",
    "#### Publish a Pipeline Returns\n",
    "\n",
    "| Field | Type | Description |\n",
    "|---|---|---|\n",
    "| id | integer | Numerical Wallaroo id of the published pipeline. |\n",
    "| pipeline version id | integer | Numerical Wallaroo id of the pipeline version published. |\n",
    "| status | string | The status of the pipeline publication.  Values include:  <ul><li>PendingPublish: The pipeline publication is about to be uploaded or is in the process of being uploaded.</li><li>Published:  The pipeline is published and ready for use.</li></ul> |\n",
    "| Engine URL | string | The URL of the published pipeline engine in the edge registry. |\n",
    "| Pipeline URL | string | The URL of the published pipeline in the edge registry. |\n",
    "| Helm Chart URL | string | The URL of the helm chart for the published pipeline in the edge registry. |\n",
    "| Helm Chart Reference | string | The help chart reference. |\n",
    "| Helm Chart Version | string | The version of the Helm Chart of the published pipeline.  This is also used as the Docker tag. |\n",
    "| Engine Config | `wallaroo.deployment_config.DeploymentConfig` | The pipeline configuration included with the published pipeline. |\n",
    "| Created At | DateTime | When the published pipeline was created. |\n",
    "| Updated At | DateTime | When the published pipeline was updated. |\n",
    "\n",
    "## Publish the Pipeline for Edge Deployment\n",
    "\n",
    "We will now publish the pipeline to our Edge Deployment Registry with the `pipeline.publish(deployment_config)` command.  `deployment_config` is an optional field that specifies the pipeline deployment.  This can be overridden by the DevOps engineer during deployment.\n",
    "\n",
    "In this example, assuming that the pipeline was saved to the variable `my_pipeline`, we would publish it to the Edge Registry already stored in the Wallaroo instance and store the pipeline publish to the variable `my_pub` with the following command:\n",
    "\n",
    "```python\n",
    "my_pub=pipeline.publish(deploy_config)\n",
    "# display the publish\n",
    "my_pub\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574281a8-9d8b-4588-b82a-acd3dcf37bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## blank space to publish the pipeline\n",
    "\n",
    "# display the publish\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04131fe1",
   "metadata": {},
   "source": [
    "## List Published Pipelines\n",
    "\n",
    "The method `wallaroo.client.list_pipelines()` shows a list of all pipelines in the Wallaroo instance, and includes the `published` field that indicates whether the pipeline was published to the registry (`True`), or has not yet been published (`False`).\n",
    "\n",
    "### List Published Pipelines Exercise\n",
    "\n",
    "List all pipelines and see which ones are published or not.  For example, if your client was saved to the variable `wl`, then the following will list the pipelines and display which ones are published.\n",
    "\n",
    "```python\n",
    "wl.list_pipelines()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791c656e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the pipelines and view which are published\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb75748e",
   "metadata": {},
   "source": [
    "## List Publishes from a Pipeline\n",
    "\n",
    "All publishes created from a pipeline are displayed with the `wallaroo.pipeline.publishes` method.  The `pipeline_version_id` is used to know what version of the pipeline was used in that specific publish.  This allows for pipelines to be updated over time, and newer versions to be sent and tracked to the Edge Deployment Registry service.\n",
    "\n",
    "### List Publishes Parameters\n",
    "\n",
    "N/A\n",
    "\n",
    "### List Publishes Returns\n",
    "\n",
    "A List of the following fields:\n",
    "\n",
    "| Field | Type | Description |\n",
    "|---|---|---|\n",
    "| id | integer | Numerical Wallaroo id of the published pipeline. |\n",
    "| pipeline_version_id | integer | Numerical Wallaroo id of the pipeline version published. |\n",
    "| engine_url | string | The URL of the published pipeline engine in the edge registry. |\n",
    "| pipeline_url | string | The URL of the published pipeline in the edge registry. |\n",
    "| created_by | string | The email address of the user that published the pipeline.\n",
    "| Created At | DateTime | When the published pipeline was created. |\n",
    "| Updated At | DateTime | When the published pipeline was updated. |\n",
    "\n",
    "### List Publishes from a Pipeline Exercise\n",
    "\n",
    "List all of the publishes from our pipeline.  For example, if our pipeline is `my_pipeline`, then we would list all publishes from the pipeline with the following:\n",
    "\n",
    "```python\n",
    "my_pipeline.publishes()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31001c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the publishes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d3669d-b3b4-4747-8ded-e8585f38a6e5",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "You have now \n",
    "\n",
    "* Created a workspace and set it as the current workspace.\n",
    "* Uploaded an ONNX model.\n",
    "* Created a Wallaroo pipeline, and set the most recent version of the uploaded model as a pipeline step.\n",
    "* Successfully send data to your pipeline for inference through the SDK and through an API call.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ecfb5f",
   "metadata": {},
   "source": [
    "## DevOps - Pipeline Edge Deployment\n",
    "\n",
    "Once a pipeline is deployed to the Edge Registry service, it can be deployed in environments such as Docker, Kubernetes, or similar container running services by a DevOps engineer.\n",
    "\n",
    "### Docker Deployment\n",
    "\n",
    "First, the DevOps engineer must authenticate to the same OCI Registry service used for the Wallaroo Edge Deployment registry.\n",
    "\n",
    "For more details, check with the documentation on your artifact service.  The following are provided for the three major cloud services:\n",
    "\n",
    "* [Set up authentication for Docker](https://cloud.google.com/artifact-registry/docs/docker/authentication)\n",
    "* [Authenticate with an Azure container registry](https://learn.microsoft.com/en-us/azure/container-registry/container-registry-authentication?tabs=azure-cli)\n",
    "* [Authenticating Amazon ECR Repositories for Docker CLI with Credential Helper](https://aws.amazon.com/blogs/compute/authenticating-amazon-ecr-repositories-for-docker-cli-with-credential-helper/)\n",
    "\n",
    "For the deployment, the engine URL is specified with the following environmental variables:\n",
    "\n",
    "```bash\n",
    "{published engine url}\n",
    "-e DEBUG=true -e OCI_REGISTRY={your registry server} \\\n",
    "-e CONFIG_CPUS=4 \\ # optional number of CPUs to use\n",
    "-e OCI_USERNAME={registry username} \\\n",
    "-e OCI_PASSWORD={registry token here} \\\n",
    "-e PIPELINE_URL={published pipeline url}\n",
    "```\n",
    "\n",
    "#### Docker Deployment Example\n",
    "\n",
    "Using our sample environment, here's sample deployment using Docker with a computer vision ML model, the same used in the [Wallaroo Use Case Tutorials Computer Vision: Retail]({{<ref \"use-case-computer-vision-retail\">}}) tutorials.\n",
    "\n",
    "```bash\n",
    "docker run -p 8080:8080 \\\n",
    "    -e DEBUG=true -e OCI_REGISTRY={your registry server} \\\n",
    "    -e CONFIG_CPUS=4 \\\n",
    "    -e OCI_USERNAME=oauth2accesstoken \\\n",
    "    -e OCI_PASSWORD={registry token here} \\\n",
    "    -e PIPELINE_URL={your registry server}/pipelines/edge-cv-retail:bf70eaf7-8c11-4b46-b751-916a43b1a555 \\\n",
    "    {your registry server}/engine:v2023.3.0-main-3707\n",
    "```\n",
    "\n",
    "### Docker Compose Deployment\n",
    "\n",
    "For users who prefer to use `docker compose`, the following sample `compose.yaml` file is used to launch the Wallaroo Edge pipeline.  This is the same used in the [Wallaroo Use Case Tutorials Computer Vision: Retail]({{<ref \"use-case-computer-vision-retail\">}}) tutorials.\n",
    "\n",
    "```yml\n",
    "services:\n",
    "  engine:\n",
    "    image: {Your Engine URL}\n",
    "    ports:\n",
    "      - 8080:8080\n",
    "    environment:\n",
    "      PIPELINE_URL: {Your Pipeline URL}\n",
    "      OCI_REGISTRY: {Your Edge Registry URL}\n",
    "      OCI_USERNAME:  {Your Registry Username}\n",
    "      OCI_PASSWORD: {Your Token or Password}\n",
    "      CONFIG_CPUS: 4\n",
    "```\n",
    "\n",
    "For example:\n",
    "\n",
    "```yml\n",
    "services:\n",
    "  engine:\n",
    "    image: sample-registry.com/engine:v2023.3.0-main-3707\n",
    "    ports:\n",
    "      - 8080:8080\n",
    "    environment:\n",
    "      PIPELINE_URL: sample-registry.com/pipelines/edge-cv-retail:bf70eaf7-8c11-4b46-b751-916a43b1a555\n",
    "      OCI_REGISTRY: sample-registry.com\n",
    "      OCI_USERNAME:  _json_key_base64\n",
    "      OCI_PASSWORD: abc123\n",
    "      CONFIG_CPUS: 4\n",
    "```\n",
    "\n",
    "#### Docker Compose Deployment Example\n",
    "\n",
    "The deployment and undeployment is then just a simple `docker compose up` and `docker compose down`.  The following shows an example of deploying the Wallaroo edge pipeline using `docker compose`.\n",
    "\n",
    "```bash\n",
    "docker compose up\n",
    "[+] Running 1/1\n",
    " ✔ Container cv_data-engine-1  Recreated                                                                                                                                                                 0.5s\n",
    "Attaching to cv_data-engine-1\n",
    "cv_data-engine-1  | Wallaroo Engine - Standalone mode\n",
    "cv_data-engine-1  | Login Succeeded\n",
    "cv_data-engine-1  | Fetching manifest and config for pipeline: sample-registry.com/pipelines/edge-cv-retail:bf70eaf7-8c11-4b46-b751-916a43b1a555\n",
    "cv_data-engine-1  | Fetching model layers\n",
    "cv_data-engine-1  | digest: sha256:c6c8869645962e7711132a7e17aced2ac0f60dcdc2c7faa79b2de73847a87984\n",
    "cv_data-engine-1  |   filename: c6c8869645962e7711132a7e17aced2ac0f60dcdc2c7faa79b2de73847a87984\n",
    "cv_data-engine-1  |   name: resnet-50\n",
    "cv_data-engine-1  |   type: model\n",
    "cv_data-engine-1  |   runtime: onnx\n",
    "cv_data-engine-1  |   version: 693e19b5-0dc7-4afb-9922-e3f7feefe66d\n",
    "cv_data-engine-1  |\n",
    "cv_data-engine-1  | Fetched\n",
    "cv_data-engine-1  | Starting engine\n",
    "cv_data-engine-1  | Looking for preexisting `yaml` files in //modelconfigs\n",
    "cv_data-engine-1  | Looking for preexisting `yaml` files in //pipelines\n",
    "```\n",
    "\n",
    "### Helm Deployment\n",
    "\n",
    "Published pipelines can be deployed through the use of helm charts.\n",
    "\n",
    "Helm deployments take up to two steps - the first step is in retrieving the required `values.yaml` and making updates to override.\n",
    "\n",
    "1. Pull the helm charts from the published pipeline.  The two fields are the Helm Chart URL and the Helm Chart version to specify the OCI .    This typically takes the format of:\n",
    "\n",
    "  ```bash\n",
    "  helm pull oci://{published.helm_chart_url} --version {published.helm_chart_version}\n",
    "  ```\n",
    "\n",
    "1. Extract the `tgz` file and copy the `values.yaml` and copy the values used to edit engine allocations, etc.  The following are **required** for the deployment to run:\n",
    "\n",
    "  ```yml\n",
    "  ociRegistry:\n",
    "    registry: {your registry service}\n",
    "    username:  {registry username here}\n",
    "    password: {registry token here}\n",
    "  ```\n",
    "\n",
    "  Store this into another file, suc as `local-values.yaml`.\n",
    "\n",
    "1. Create the namespace to deploy the pipeline to.  For example, the namespace `wallaroo-edge-pipeline` would be:\n",
    "\n",
    "  ```bash\n",
    "  kubectl create -n wallaroo-edge-pipeline\n",
    "  ```\n",
    "\n",
    "1. Deploy the `helm` installation with `helm install` through one of the following options:\n",
    "    1. Specify the `tgz` file that was downloaded and the local values file.  For example:\n",
    "\n",
    "        ```bash\n",
    "        helm install --namespace {namespace} --values {local values file} {helm install name} {tgz path}\n",
    "        ```\n",
    "\n",
    "    1. Specify the expended directory from the downloaded `tgz` file.\n",
    "\n",
    "        ```bash\n",
    "        helm install --namespace {namespace} --values {local values file} {helm install name} {helm directory path}\n",
    "        ```\n",
    "\n",
    "    1. Specify the Helm Pipeline Helm Chart and the Pipeline Helm Version.\n",
    "\n",
    "        ```bash\n",
    "        helm install --namespace {namespace} --values {local values file} {helm install name} oci://{published.helm_chart_url} --version {published.helm_chart_version}\n",
    "        ```\n",
    "\n",
    "1. Once deployed, the DevOps engineer will have to forward the appropriate ports to the `svc/engine-svc` service in the specific pipeline.  For example, using `kubectl port-forward` to the namespace `ccfraud` that would be:\n",
    "\n",
    "    ```bash\n",
    "    kubectl port-forward svc/engine-svc -n ccfraud01 8080 --address 0.0.0.0`\n",
    "    ```\n",
    "\n",
    "The following code segment generates a `docker compose` template based on the previously published pipeline, assuming our publish was listed as `my_pub`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f13ce16",
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_compose = f'''\n",
    "services:\n",
    "  engine:\n",
    "    image: {my_pub.engine_url}\n",
    "    ports:\n",
    "      - 8080:8080\n",
    "    environment:\n",
    "      PIPELINE_URL: {my_pub.pipeline_url}\n",
    "      OCI_USERNAME: YOUR USERNAME \n",
    "      OCI_PASSWORD: YOUR PASSWORD OR TOKEN\n",
    "      OCI_REGISTRY: ghcr.io\n",
    "      CONFIG_CPUS: 0.5\n",
    "'''\n",
    "\n",
    "print(docker_compose)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241aa8bc",
   "metadata": {},
   "source": [
    "### Docker Compose Deployment Exercise\n",
    "\n",
    "Use the `docker compose up` command on your own `compose.yaml` using the sample above, replacing the `OCI_USERNAME` and `OCI_PASSWORD` with the values provided by your instructor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77126620",
   "metadata": {},
   "source": [
    "## Edge Deployed Pipeline API Endpoints\n",
    "\n",
    "Once deployed, we can check the pipelines and models available.  We'll use a `curl` command, but any HTTP based request will work the same way.\n",
    "\n",
    "### Pipelines Endpoint\n",
    "\n",
    "The endpoint `/pipelines` returns:\n",
    "\n",
    "* **id** (*String*):  The name of the pipeline.\n",
    "* **status** (*String*):  The status as either `Running`, or `Error` if there are any issues.\n",
    "\n",
    "#### Pipelines Endpoint Exercise\n",
    "\n",
    "Use the pipeline \n",
    "\n",
    "For this example, the deployment is made on a machine called `testboy.local`.  Replace this URL with the URL of you edge deployment.\n",
    "\n",
    "Assuming you are in the same directory as the `compose.yaml` file, that command would be:\n",
    "\n",
    "```bash\n",
    "docker compose up\n",
    "```\n",
    "\n",
    "Later, to shut down the Edge deployed pipeline the command is:\n",
    "\n",
    "```bash\n",
    "docker compose down\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44605c62",
   "metadata": {},
   "source": [
    "## Edge Deployed Pipeline API Endpoints\n",
    "\n",
    "Once deployed, we can check the pipelines and models available.  We'll use a `curl` command, but any HTTP based request will work the same way.\n",
    "\n",
    "### Pipelines Endpoints\n",
    "\n",
    "The endpoint `/pipelines` returns:\n",
    "\n",
    "* **id** (*String*):  The name of the pipeline.\n",
    "* **status** (*String*):  The status as either `Running`, or `Error` if there are any issues.\n",
    "\n",
    "For this example, the deployment is made on a machine called `testboy.local`.  Replace this URL with the URL of you edge deployment.\n",
    "\n",
    "#### Pipelines Endpoints Exercise\n",
    "\n",
    "Use the following `curl` command to view the pipeline data.  For example, if the pipeline was deployed on `localhost`, then the command would be:\n",
    "\n",
    "```bash\n",
    "!curl locahost:8080/pipelines\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35de3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# blank space to run the command \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d814f93",
   "metadata": {},
   "source": [
    "### Models Endpoints\n",
    "\n",
    "The endpoint `/models` returns a List of models with the following fields:\n",
    "\n",
    "* **name** (*String*): The model name.\n",
    "* **sha** (*String*): The sha hash value of the ML model.\n",
    "* **status** (*String*):  The status of either Running or Error if there are any issues.\n",
    "* **version** (*String*):  The model version.  This matches the version designation used by Wallaroo to track model versions in UUID format.\n",
    "\n",
    "#### Models Endpoints Exercise\n",
    "\n",
    "Use the following `curl` command to view the models data.  For example, if the pipeline was deployed on `localhost`, then the command would be:\n",
    "\n",
    "```bash\n",
    "!curl locahost:8080/models\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78b877f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# blank space to run the command \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc16183",
   "metadata": {},
   "source": [
    "### Edge Deployed Inference\n",
    "\n",
    "The inference endpoint takes the following pattern:\n",
    "\n",
    "* `/pipelines/{pipeline-name}`:  The `pipeline-name` is the same as returned from the [`/pipelines`](#list-pipelines) endpoint as `id`.\n",
    "\n",
    "Wallaroo inference endpoint URLs accept the following data inputs through the `Content-Type` header:\n",
    "\n",
    "* `Content-Type: application/vnd.apache.arrow.file`: For Apache Arrow tables.\n",
    "* `Content-Type: application/json; format=pandas-records`: For pandas DataFrame in record format.\n",
    "* `Content-Type: application/json`: JSON.\n",
    "\n",
    "The `Accept` header will determine what kind format of the data is returned.\n",
    "\n",
    "* `Accept: application/json`: Returns a JSON object in the following format.\n",
    "\n",
    "* **check_failures** (*List[Integer]*): Any Validations that failed.  For more information, see [Wallaroo SDK Essentials Guide: Pipeline Management: Anomaly Testing](https://staging.docs.wallaroo.ai/20230201/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-essentials-pipelines/wallaroo-sdk-essentials-pipeline/#anomaly-testing)\n",
    "* **elapsed** (*List[Integer]*): A List of time in nanoseconds for:\n",
    "  * [0]: The time to serialize the input.\n",
    "  * [1...n]: How long each step took.\n",
    "* **model_name** (*String*): The name of the model.\n",
    "* **model_version** (*String*): The UUID identifier of the model version from Wallaroo.\n",
    "* **original_data** (*List*): The original submitted data.\n",
    "* **outputs** (*List*): A List of outputs with each output field corresponding to the input.  This is in the format for each data type returned:\n",
    "  * **{data-type}**: The data type being returned.\n",
    "    * **data** (*List*): The data from this data type.\n",
    "    * **dim** (*List*): The shape of the data for this data type.\n",
    "* **dim** (*List*): The shape of the data received.\n",
    "* **pipeline_name** (*String*): The name of the pipeline.\n",
    "* **shadow_data** (*List*): Any data returned from a shadow deployed model.  For more information, see [Wallaroo SDK Essentials Guide: Pipeline Management: Shadow Deployments](https://staging.docs.wallaroo.ai/20230201/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-essentials-pipelines/wallaroo-sdk-essentials-pipeline/#pipeline-shadow-deployments).\n",
    "* **time** (*Integer*): The Epoch time of the inference.\n",
    "  \n",
    "#### Edge Deployed Inference Exercise\n",
    "\n",
    "Use the Python `request` library to perform an inference through the Edge deployed pipeline.  For this example, assumming that the host is `localhost` and the pipeline was named `houseprice-estimator`, this would be:\n",
    "\n",
    "```python\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# set the content type and accept headers\n",
    "headers = {\n",
    "    'Content-Type': 'application/json; format=pandas-records'\n",
    "}\n",
    "\n",
    "# Submit arrow file\n",
    "dataFile=\"../data/test_data.df.json\"\n",
    "\n",
    "data = json.load(open(dataFile))\n",
    "\n",
    "host = 'http://testboy.local:8080'\n",
    "\n",
    "deployurl = f'{host}/pipelines/houseprice-estimator'\n",
    "\n",
    "response = requests.post(\n",
    "                    deployurl, \n",
    "                    headers=headers, \n",
    "                    json=data, \n",
    "                    verify=True\n",
    "                )\n",
    "\n",
    "# # display the first 10 results back\n",
    "display(response.json()[0]['outputs'][0]['Float']['data'][0:10])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbdbe00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform inference through the deployed pipeline\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
